---
title: Attention is Not What You Need
date: 2026-01-02
tags:
  - ml
  - papers
draft: true
---

[Original Paper](https://arxiv.org/abs/2512.19428)

### Summary:
This paper views attention as:
> a particular instance of _tensor lifting_: a hidden vector is mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. 

As an alternative, they propose "an _attention-free_ sequence model built around _Grassmann flows_." 
### Thoughts:
To me, this is one of the most exciting papers in recent times. It relates very closely to a lot of the ideas I've been floating around regarding using multivectors and geometric algebra instead of being constrained to simple vectors as is standard practice.

